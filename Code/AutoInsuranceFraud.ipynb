{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "This notebook shows how to use Amazon Sagemaker Processsing, Data Wrangler and Amazon Glue Data Brew to prepare the data. \n",
    "\n",
    "First, we process the raw dataset to prepare the features and extract the interactions in the dataset that will be used to construct the graph. \n",
    "\n",
    "Then, we create a launch a training job using the SageMaker framework estimator to train a XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Initial Setup\n",
    "\n",
    "The below code is used to get the S3 Bucket name configured for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Obtaining file:///root/AutoInsuranceFraudDetection/Code/sagemaker_graph_fraud_detection\n",
      "Installing collected packages: sagemaker-graph-fraud-detection\n",
      "  Attempting uninstall: sagemaker-graph-fraud-detection\n",
      "    Found existing installation: sagemaker-graph-fraud-detection 1.0\n",
      "    Uninstalling sagemaker-graph-fraud-detection-1.0:\n",
      "      Successfully uninstalled sagemaker-graph-fraud-detection-1.0\n",
      "  Running setup.py develop for sagemaker-graph-fraud-detection\n",
      "Successfully installed sagemaker-graph-fraud-detection-1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "S3 bucket name:  sagemaker-us-east-1-367858208265\n"
     ]
    }
   ],
   "source": [
    "!bash setup.sh\n",
    "import sagemaker\n",
    "from sagemaker_graph_fraud_detection import config\n",
    "\n",
    "role = config.role\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()  \n",
    "print(\"S3 bucket name: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Sagemaker Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#container to run the processing. The ecr_repository_uri will vary depending on the region. The \"source\" field is used for the dataset and the \"destination\" is used to store the prepared data\n",
    "ecr_repository_uri = \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\"\n",
    "source = 's3://'+bucket+'/AutoInsuranceFraudDetection/DataSet/insurance_claims.csv'\n",
    "destination = 's3://'+bucket+'/AutoInsuranceFraudDetection/Results/DataProcessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AutoInsuranceFraudProcessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile AutoInsuranceFraudProcessing.py\n",
    "#This block of code generates a file \"AutoInsuranceFraudProcessing.py\" which has the code to process the data\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #get arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    #get the input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"insurance_claims.csv\")\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df)\n",
    "    print(df.head())\n",
    "\n",
    "    #replacing ? with nan for the columns\n",
    "    df['police_report_available']=df['police_report_available'].replace('?',np.nan)\n",
    "    df['collision_type']=df['collision_type'].replace('?',np.nan)\n",
    "    df['property_damage']=df['property_damage'].replace('?',np.nan)\n",
    "    \n",
    "    #droping rows with nan\n",
    "    df=df.dropna(subset=['police_report_available', 'collision_type','property_damage'])\n",
    "    \n",
    "    #dropping the unnecessary rows\n",
    "    df=df.drop(['months_as_customer','policy_number','policy_bind_date','policy_csl','auto_year','auto_model','insured_hobbies','insured_zip'],axis=1)\n",
    "    \n",
    "    #now deal with the categorical features\n",
    "    le=LabelEncoder()\n",
    "    for i in list(df.columns):\n",
    "        if df[i].dtype=='object':\n",
    "            df[i]=le.fit_transform(df[i])\n",
    "    \n",
    "    #final preprocessed data\n",
    "    print(df.head())\n",
    "    train_features_output_path = os.path.join(\"/opt/ml/processing/output\", \"preprocessed_data.csv\")\n",
    "    df.to_csv(train_features_output_path, index=False)\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `AutoInsuranceFraudProcessing.py` performs data preprocessing transformations on the raw data. The preproceesing involes replacing values, droping rows, dropping columns and categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-09-08-12-41-10-629\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-367858208265/AutoInsuranceFraudDetection/DataSet/insurance_claims.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-367858208265/sagemaker-scikit-learn-2021-09-08-12-41-10-629/input/code/AutoInsuranceFraudProcessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'preprocessed_data.csv', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-367858208265/AutoInsuranceFraudDetection/Results/DataProcessing', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..............................\n",
      "\u001b[34mReceived arguments Namespace()\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/insurance_claims.csv\n",
      "   months_as_customer  age  policy_number  ... auto_model auto_year fraud_reported\u001b[0m\n",
      "\u001b[34m0                 328   48         521585  ...        92x      2004              Y\u001b[0m\n",
      "\u001b[34m1                 228   42         342868  ...       E400      2007              Y\u001b[0m\n",
      "\u001b[34m2                 134   29         687698  ...        RAM      2007              N\u001b[0m\n",
      "\u001b[34m3                 256   41         227811  ...      Tahoe      2014              Y\u001b[0m\n",
      "\u001b[34m4                 228   44         367455  ...        RSX      2009              N\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 39 columns]\n",
      "    age  policy_state  ...  auto_make  fraud_reported\u001b[0m\n",
      "\u001b[34m0    48             2  ...         10               1\u001b[0m\n",
      "\u001b[34m2    29             2  ...          4               0\u001b[0m\n",
      "\u001b[34m5    39             2  ...         10               1\u001b[0m\n",
      "\u001b[34m8    33             0  ...         12               0\u001b[0m\n",
      "\u001b[34m11   61             2  ...          1               0\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 31 columns]\u001b[0m\n",
      "\u001b[34mdone\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=ecr_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.m4.2xlarge')\n",
    "\n",
    "script_processor.run(code='AutoInsuranceFraudProcessing.py',\n",
    "                     inputs=[ProcessingInput(source=source,\n",
    "                                             destination='/opt/ml/processing/input')],\n",
    "                     outputs=[ProcessingOutput(output_name=\"preprocessed_data.csv\", destination=destination,\n",
    "                                               source='/opt/ml/processing/output')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket to see the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OutputName': 'preprocessed_data.csv', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-367858208265/AutoInsuranceFraud/Results/DataProcessing', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}\n",
      "s3://sagemaker-us-east-1-367858208265/AutoInsuranceFraud/Results/DataProcessing\n"
     ]
    }
   ],
   "source": [
    "preprocessing_job_description = script_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    print(output)\n",
    "    preprocessed_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    print(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are automatically saved and SageMaker stores the trained model and evaluation results to a location in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Sagemaker Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# read the prepared data from S3. Enter any of the Results processed file S3 location\n",
    "source = 's3://sagemaker-us-east-1-367858208265/Results/DataWrangler/output_1631272206/part-00000-d3369d58-6799-4d9f-91bd-0f0159be50b4-c000.csv'\n",
    "df = pd.read_csv(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a feature group, you can also create the metadata for the feature group, such as a short description, storage configuration, features for identifying each record, and the event time, as well as tags to store information such as the author, data source, version, and more. Since we do not have any such column, we are adding two extra columns called Fraud_ID and Fraud_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_state</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_sex</th>\n",
       "      <th>insured_education_level</th>\n",
       "      <th>insured_occupation</th>\n",
       "      <th>insured_relationship</th>\n",
       "      <th>...</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_make</th>\n",
       "      <th>fraud_reported</th>\n",
       "      <th>Fraud_ID</th>\n",
       "      <th>Fraud_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>521585</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1406.91</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>71610</td>\n",
       "      <td>6510</td>\n",
       "      <td>13020</td>\n",
       "      <td>52080</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.631333e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>687698</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1413.14</td>\n",
       "      <td>5000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34650</td>\n",
       "      <td>7700</td>\n",
       "      <td>3850</td>\n",
       "      <td>23100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.631333e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>227811</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1415.74</td>\n",
       "      <td>6000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>63400</td>\n",
       "      <td>6340</td>\n",
       "      <td>6340</td>\n",
       "      <td>50720</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1002</td>\n",
       "      <td>1.631333e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>367455</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1583.91</td>\n",
       "      <td>6000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6500</td>\n",
       "      <td>1300</td>\n",
       "      <td>650</td>\n",
       "      <td>4550</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1003</td>\n",
       "      <td>1.631333e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>104594</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1351.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>64100</td>\n",
       "      <td>6410</td>\n",
       "      <td>6410</td>\n",
       "      <td>51280</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1004</td>\n",
       "      <td>1.631333e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  policy_number  policy_state  policy_deductable  policy_annual_premium  \\\n",
       "0   48         521585             2               1000                1406.91   \n",
       "1   29         687698             2               2000                1413.14   \n",
       "2   41         227811             0               2000                1415.74   \n",
       "3   44         367455             0               1000                1583.91   \n",
       "4   39         104594             2               1000                1351.10   \n",
       "\n",
       "   umbrella_limit  insured_sex  insured_education_level  insured_occupation  \\\n",
       "0               0            1                        4                   2   \n",
       "1         5000000            0                        6                  11   \n",
       "2         6000000            0                        6                   1   \n",
       "3         6000000            1                        0                  11   \n",
       "4               0            0                        6                  12   \n",
       "\n",
       "   insured_relationship  ...  witnesses  police_report_available  \\\n",
       "0                     0  ...          2                        1   \n",
       "1                     3  ...          3                        0   \n",
       "2                     4  ...          2                        0   \n",
       "3                     4  ...          1                        0   \n",
       "4                     4  ...          2                        0   \n",
       "\n",
       "   total_claim_amount  injury_claim  property_claim  vehicle_claim  auto_make  \\\n",
       "0               71610          6510           13020          52080         10   \n",
       "1               34650          7700            3850          23100          4   \n",
       "2               63400          6340            6340          50720          3   \n",
       "3                6500          1300             650           4550          0   \n",
       "4               64100          6410            6410          51280         10   \n",
       "\n",
       "   fraud_reported  Fraud_ID    Fraud_time  \n",
       "0             1.0      1000  1.631333e+09  \n",
       "1             0.0      1001  1.631333e+09  \n",
       "2             1.0      1002  1.631333e+09  \n",
       "3             0.0      1003  1.631333e+09  \n",
       "4             1.0      1004  1.631333e+09  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add unique ID and event time for features store\n",
    "df['Fraud_ID'] = df.index + 1000\n",
    "current_time_sec = int(round(time.time()))\n",
    "df['Fraud_time'] = pd.Series([current_time_sec]*len(df), dtype=\"float64\")\n",
    "df=df.drop(['_c0'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize necessary variables\n",
    "import boto3\n",
    "region = sagemaker.Session().boto_region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the correspongin Python data type in the Pandas dataframe when it’s ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker.feature_store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89238efdee86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#configure the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_group\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureGroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfraud_fg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"auto-fraud\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfraud_feature_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureGroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfraud_fg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfraud_feature_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_feature_definitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker.feature_store'"
     ]
    }
   ],
   "source": [
    "#configure the features\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "fraud_fg_name = f\"auto-fraud\"\n",
    "fraud_feature_group = FeatureGroup(name=fraud_fg_name, sagemaker_session=sess)\n",
    "fraud_feature_group.load_feature_definitions(data_frame=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record indentifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using s3://sagemaker-us-east-1-367858208265/DataSet/insurance_claims.csv\n",
      "Create \"fraud\" feature group: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "record_identifier_feature_name = \"Fraud_ID\"\n",
    "event_time_feature_name = \"Fraud_time\"\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "try:\n",
    "    print(f\"\\n Using s3://{bucket}/DataSet/insurance_claims.csv\")\n",
    "    fraud_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/DataSet/insurance_claims.csv\",\n",
    "        record_identifier_name='Fraud_ID',\n",
    "        event_time_feature_name='Fraud_time',\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True,\n",
    "    )\n",
    "    print(f'Create \"fraud\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(f\"Using existing feature group: {fraud_fg_name}\")\n",
    "    else:\n",
    "        raise (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until feature group creation has fully completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup auto-fraud successfully created.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=fraud_feature_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='auto-fraud', sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f6bc411b910>, max_workers=3, max_processes=1, _async_result=<multiprocess.pool.MapResult object at 0x7f6bc4d86950>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_feature_group.ingest(data_frame=df, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for offline store data to become available\n",
    "This usually takes 5-8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n",
      "Waiting for data in offline store...\n"
     ]
    }
   ],
   "source": [
    "fraud_feature_group_s3_prefix = (\n",
    "    f\"ResultSet/FeatureStore/offline-store/data\"\n",
    ")\n",
    "\n",
    "offline_store_contents = None\n",
    "while offline_store_contents is None:\n",
    "    objects_in_bucket = s3_client.list_objects(\n",
    "        Bucket=bucket, Prefix=fraud_feature_group_s3_prefix\n",
    "    )\n",
    "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
    "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
    "    else:\n",
    "        print(\"Waiting for data in offline store...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "print(\"\\nData available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is available in the offline store, it will automatically be cataloged and loaded into an Athena table (this is done by default, but can be turned off). In order to build our training and test datasets, you will submit a SQL query to join the the Claims and Customers tables created in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fraud_feature_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-48ede1777190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfraud_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfraud_feature_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mathena_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfraud_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfraud_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fraud_feature_group' is not defined"
     ]
    }
   ],
   "source": [
    "fraud_query = fraud_feature_group.athena_query()\n",
    "\n",
    "fraud_table = fraud_query.table_name\n",
    "\n",
    "feature_columns = list(set(df.columns) ^ set(df.columns))\n",
    "feature_columns_string = \", \".join(f'\"{c}\"' for c in feature_columns)\n",
    "feature_columns_string = f'\"{fraud_table}\".Fraud_ID as Fraud_ID, ' + feature_columns_string\n",
    "\n",
    "query_string = f\"\"\"\n",
    "SELECT *\n",
    "FROM auto-fraud-1631293997\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'claims_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-55d261c9913e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclaims_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"s3://{bucket}/Results/Train/query_results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclaims_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclaims_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'claims_query' is not defined"
     ]
    }
   ],
   "source": [
    "fraud_query.run(query_string=query_string, output_location=f\"s3://{bucket}/Results/Train/query_results\")\n",
    "fraud_query.wait()\n",
    "dataset = fraud_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data/claims_customer.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"fraud\"] + list(dataset.drop([\"fraud\", \"policy_id\"], axis=1).columns)\n",
    "%store col_order\n",
    "\n",
    "train = dataset.sample(frac=0.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write train, test data to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "dataset.to_csv(\"data/dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"data/train.csv\", Bucket=bucket, Key=f\"Results/data/train/train.csv\"\n",
    ")\n",
    "s3_client.upload_file(Filename=\"data/test.csv\", Bucket=bucket, Key=f\"Results/data/test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and test datasets have been persisted in S3, you can start training a model by defining which SageMaker Estimator you’d like to use. For this guide, you will use the XGBoost Open Source Framework to train your model. This estimator is accessed via the SageMaker SDK, but mirrors the open source version of the XGBoost Python package. Any functioanlity provided by the XGBoost Python package can be implemented in your training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables used for parameterizing the notebook run\n",
    "estimator_output_path = f\"s3://{bucket}/Results/training_jobs\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "bias_report_1_output_path = f\"s3://{bucket}/Results/clarify-output/bias_1\"\n",
    "\n",
    "\n",
    "xgb_model_name = \"xgb-insurance-claims-fraud-model\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\"\n",
    "batch_transform_instance_count = 1\n",
    "batch_transform_instance_type = \"ml.c5.xlarge\"\n",
    "claify_instance_count = 1\n",
    "clairfy_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters\n",
    "These are the parameters which will be sent to our training script in order to train the model. Although they are all defined as “hyperparameters” here, they can encompass XGBoost’s Learning Task Parameters, Tree Booster Parameters, or any other parameters you’d like to configure for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the estimator\n",
    "If you want to explore the breadth of functionailty offered by the SageMaker XGBoost Framework you can read about all the configuration parameters by referencing the inhereting classes. The XGBoost class inherets from the Framework class and Framework inherets from the EstimatorBase class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"xgboost_starter_script.py\",\n",
    "    output_path=estimator_output_path,\n",
    "    code_location=estimator_output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker_role,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.0-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'training_job_1_name' not in locals():\n",
    "\n",
    "    xgb_estimator.fit(inputs = {'train': train_data_uri})\n",
    "    training_job_1_name = xgb_estimator.latest_training_job.job_name\n",
    "    %store training_job_1_name\n",
    "\n",
    "else:\n",
    "    print(f'Using previous training job: {training_job_1_name}')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
